
dataset:
  root: "file:D:/data/sign-languages/phoenix"
  train_shards_url: "shards/shard_{000000..000001}.tar"
  val_shards_url: "shards/shard_000002.tar"
  test_shards_url: "shards/shard_000002.tar"

preprocessing:
  use_windows: true
  window_size: 300
  window_stride: 240
  transforms_pipeline: "norm+flatten-pose"

backbone:
  name: 'mstcn'
  multilayer_output: true
#  checkpoint_path: "D:/data/sign-languages/outs/sls/alpha1/checkpoints/dgs_io_451908/epoch=51-step=32812.ckpt"
  args:
    in_channels: 130
    hidden_channels: 64
    n_stages: 4
    n_layers: 10
    out_channels: 2

target:
  offsets: false
  encoder:
    name: 'actionness'
    args: {}
  decoder:
    name: 'actionness'
    args: {}

training:
  batch_size: 8
  n_workers: 0
  criterion: 'multi-layer+ce+smoothing'
  learning_rate: 2e-4
  n_epochs: 200
  gradient_clipping: 0.0
  early_stopping_patience: 30
  criterion_use_weights: false
  skip_training: false
  skip_testing: false

experiment:
  name: "phoenix_io"
  debug: false
  out_dir: "D:/data/sign-languages/outs/sls/alpha1_local"